{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face-masks-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itamarco/face-mask-detector/blob/master/face_masks_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlNvemZY7eGN"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score\n",
        "import pathlib\n",
        "\n",
        "import imutils\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agtyF6WeT7NJ"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfDZ_DOi8COc",
        "outputId": "13f9f0ee-75aa-44a2-dded-faf1e23e9746"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfLltKFEWSdg",
        "outputId": "dce839cb-5d9d-43c3-b941-6cf20a6bcf1c"
      },
      "source": [
        "%cd 'drive/MyDrive/face-mask-dataset/'\n",
        " \n",
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/face-mask-dataset\n",
            "dataset  face-model-t1.h5  with_mask.zip  without_mask.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOT741fhUaWi"
      },
      "source": [
        "Unzip train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSe6bAJWb_9"
      },
      "source": [
        "!unzip -q ./with_mask.zip -d ./dataset\n",
        "!unzip -q ./without_mask.zip -d ./dataset\n",
        "\n",
        "!echo with_mask images: `ls ./dataset/with_mask | wc -l`\n",
        "!echo without_mask images: `ls ./dataset/without_mask | wc -l`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRow-Y9zwu-r"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTjzcwUmwz4f"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "EPOCHS = 10\n",
        "IMG_HEIGHT = 140\n",
        "IMG_WIDTH = 140"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRBN9dHnkOYg"
      },
      "source": [
        "## Image Data Generation/Augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ry6JJLj02x",
        "outputId": "a31be9a5-87b5-46d3-82c8-ae1d86c45fca"
      },
      "source": [
        "DATA_DIR = './dataset'\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    validation_split=0.3,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_ds = datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "\n",
        "val_ds = datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    subset='validation'\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2684 images belonging to 2 classes.\n",
            "Found 1149 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H_LvY-DXEyS"
      },
      "source": [
        "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training the model. \n",
        "\n",
        "`Dataset.prefetch()` overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYiceeGekRid"
      },
      "source": [
        "##Build the network\n",
        "\n",
        "This is a convolution network:\n",
        "* Two pairs of Conv and MaxPool layers to extract features from the dataset.\n",
        "* Flatten and Dropout layer to convert the data in 1D and ensure overfitting.\n",
        "* Two Dense layers for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKN77L5Ti8G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750ac8ad-b155-432a-d978-ca228f897fdd"
      },
      "source": [
        "model = Sequential([\n",
        "    layers.Conv2D(100, (3,3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    layers.Conv2D(100, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(50, activation='relu'),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 138, 138, 100)     2800      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 69, 69, 100)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 67, 67, 100)       90100     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 33, 33, 100)       0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 108900)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 108900)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                5445050   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 5,538,052\n",
            "Trainable params: 5,538,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ges6ImHqkXiI"
      },
      "source": [
        "Initialize a callback checkpoint to keep saving best model after each epoch while training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "829Oto9ekAUb"
      },
      "source": [
        "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_UepGVokf_V"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byao7M8CkroP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc7af31-56ba-4e50-a878-750128e8a86b"
      },
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[]\n",
        ")\n",
        "model.save('face-mask-model.h5')\n",
        "print('Training is Done!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/269 [=====================>........] - ETA: 3:01 - loss: 0.6047 - acc: 0.6424"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "269/269 [==============================] - 1017s 4s/step - loss: 0.5652 - acc: 0.6774 - val_loss: 0.4409 - val_acc: 0.7972\n",
            "Epoch 2/10\n",
            "269/269 [==============================] - 283s 1s/step - loss: 0.3406 - acc: 0.8545 - val_loss: 0.2398 - val_acc: 0.9104\n",
            "Epoch 3/10\n",
            "269/269 [==============================] - 280s 1s/step - loss: 0.3261 - acc: 0.8636 - val_loss: 0.3383 - val_acc: 0.8729\n",
            "Epoch 4/10\n",
            "269/269 [==============================] - 280s 1s/step - loss: 0.3302 - acc: 0.8604 - val_loss: 0.2447 - val_acc: 0.9156\n",
            "Epoch 5/10\n",
            "269/269 [==============================] - 282s 1s/step - loss: 0.3030 - acc: 0.8742 - val_loss: 0.2288 - val_acc: 0.9060\n",
            "Epoch 6/10\n",
            "269/269 [==============================] - 281s 1s/step - loss: 0.2952 - acc: 0.8845 - val_loss: 0.2184 - val_acc: 0.9121\n",
            "Epoch 7/10\n",
            "269/269 [==============================] - 279s 1s/step - loss: 0.3448 - acc: 0.8609 - val_loss: 0.2080 - val_acc: 0.9260\n",
            "Epoch 8/10\n",
            "269/269 [==============================] - 281s 1s/step - loss: 0.2682 - acc: 0.8850 - val_loss: 0.2035 - val_acc: 0.9373\n",
            "Epoch 9/10\n",
            "269/269 [==============================] - 284s 1s/step - loss: 0.2805 - acc: 0.8860 - val_loss: 0.2506 - val_acc: 0.9138\n",
            "Epoch 10/10\n",
            "269/269 [==============================] - 282s 1s/step - loss: 0.2699 - acc: 0.8927 - val_loss: 0.2142 - val_acc: 0.9156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjDLIDnvNdB"
      },
      "source": [
        ""
      ]
    }
  ]
}